---
# Default variables that are defined by the playbook that will be deploying these roles
become_override: False
ocp_username: opentlc-mgr
silent: False

user_name: null
project_name: null

# The first user number to start with when creating projects
user_count_start: 1
# The last user number to start with when creating projects
# "num_users" is a global variable that specifies the total number of users AVAILABLE in the cluster
user_count_end: "{{user_count_start|int + num_users|int}}"

# Rook Ceph Info
# These variables will be initialized by the role ocp4-workload-rhte-analytics_data_ocp_infra
# The Rook Ceph RGW service ClusterIP in the rook-ceph namespace
rgw_service_ip: null
# The Rook Ceph RGW service port in the rook-ceph namespace
rgw_service_port: null
# Full RGW endpoint url that can be used as the S3 endpoint
rgw_endpoint_url: "http://{{ rgw_service_ip }}:{{ rgw_service_port }}"

# Open Data Hub
# Deploy the Open Data Hub CustomResource in the user project space,
#   after the project is setup and ODH ClusterServiceVersion has finished
deploy_odh_cr: false
# Open Data Hub operator image
odh_operator_image_repo: "quay.io/opendatahub/opendatahub-operator:v0.5.2"
# Registry and repistory for AICoE-JupyterHub images
# EXAMPLE: podman pull {{ jupyterhub_image_registry }}/{{ jupyterhub_image_repository }}:IMAGE:TAG
jupyterhub_image_registry: 'quay.io'
jupyterhub_image_repository: 'odh-jupyterhub'
jupyterhub_img_tag: '3.0.7-df59c25'

# Custom notebook image source that will be used for the workshop
workshop_jupyter_notebook_imagestream_image: "quay.io/opendatahub/spark-cluster-image:spark-2.3.2_hadoop-2.8.5"
# Imagestream name for the custom workshop image
workshop_jupyter_notebook_imagestream_name: s2i-spark-scipy-notebook
# Imagestream tag for the custom workshop image
workshop_jupyter_notebook_imagestream_tag: "3.6"

# Command separated string list each git repo url to preload on the notebook pod when it spawns
workshop_preload_repos: "https://gitlab.com/rimolive/data-engineering-workshop.git"

# Amount of memory for the JupyterHub server
jupyterhub_memory: "1Gi"
# Amount of memory for the spawned Jupyter Notebook pods
jupyter_notebook_memory: "2Gi"

# Image to use for the Spark Cluster
spark_node_image: "quay.io/llasmith/openshift-spark:spark-2.3.2_hadoop-2.8.5"
# Amount of resources to allocate to each Spark master node
# Number of Spark master nodes
spark_master_count: 1
spark_master_node_memory_limit: "1Gi"
spark_master_node_memory_request: "512Mi"
spark_master_node_cpu_limit: 1
spark_master_node_cpu_request: 500m

# Number of Spark worker nodes
# Amount of resources to allocate to each Spark worker node
spark_worker_count: 2
spark_worker_node_memory_limit: "2Gi"
spark_worker_node_memory_request: "1Gi"
spark_worker_node_cpu_limit: 1
spark_worker_node_cpu_request: 500m

# Path to append to env var PYTHONPATH for pyspark module
spark_pythonpath: "/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip"
# PySpark submit args to be set as the env var SPARK_SUBMIT_ARGS
spark_submit_args: "--conf spark.cores.max=1 --conf spark.executor.instances=1 --conf spark.executor.memory=4G --conf spark.executor.cores=1 --conf spark.driver.memory=2G --packages com.amazonaws:aws-java-sdk:1.8.0,org.apache.hadoop:hadoop-aws:2.8.5 pyspark-shell"
